import streamlit as st
import cv2
import time
import threading
from streamlit_extras.add_vertical_space import add_vertical_space
from streamlit_webrtc import webrtc_streamer, WebRtcMode
import pygame.midi
from cvzone.HandTrackingModule import HandDetector
import av

# üéπ Initialize Pygame MIDI
pygame.midi.init()
player = pygame.midi.Output(0)
player.set_instrument(0)  # 0 = Acoustic Grand Piano

# üéê Initialize Hand Detector
detector = HandDetector(detectionCon=0.8)

with st.sidebar:
    st.title('ChordSign')
    st.markdown('''
    ## About
    This app uses opencv and streamlit to detect hand gestures and convert them into musical chords.:
    - [Streamlit](https://streamlit.io/)
    - [OpenCV](https://opencv.org/)
    ''')
    add_vertical_space(3)

chords = {
    "left": {
        "thumb": [62, 66, 69],   # D Major (D, F#, A)
        "index": [64, 67, 71],   # E Minor (E, G, B)
        "middle": [66, 69, 73],  # F# Minor (F#, A, C#)
        "ring": [67, 71, 74],    # G Major (G, B, D)
        "pinky": [69, 73, 76]    # A Major (A, C#, E)
    },
    "right": {
        "thumb": [62, 66, 69],   # D Major (D, F#, A)
        "index": [64, 67, 71],   # E Minor (E, G, B)
        "middle": [66, 69, 73],  # F# Minor (F#, A, C#)
        "ring": [67, 71, 74],    # G Major (G, B, D)
        "pinky": [69, 73, 76]    # A Major (A, C#, E)
    }
}

SUSTAIN_TIME = 2.0
prev_states = {hand: {finger: 0 for finger in chords[hand]} for hand in chords}

# üéµ Function to Play a Chord
def play_chord(chord_notes):
    for note in chord_notes:
        player.note_on(note, 127)  # Note on

# üéµ Function to Stop a Chord After a Delay
def stop_chord_after_delay(chord_notes):
    time.sleep(SUSTAIN_TIME)  # Sustain for specified time
    for note in chord_notes:
        player.note_off(note, 127)  # Stop playing

# Video frame callback function
def video_frame_callback(frame):
    global prev_states
    
    img = frame.to_ndarray(format="bgr24")
    hands, img = detector.findHands(img, draw=True)
    
    if hands:
        for hand in hands:
            hand_type = "left" if hand["type"] == "Left" else "right"
            fingers = detector.fingersUp(hand)
            finger_names = ["thumb", "index", "middle", "ring", "pinky"]
            for i, finger in enumerate(finger_names):
                if finger in chords[hand_type]:  # Only check assigned chords
                    if fingers[i] == 1 and prev_states[hand_type][finger] == 0:
                        play_chord(chords[hand_type][finger])  # Play chord
                    elif fingers[i] == 0 and prev_states[hand_type][finger] == 1:
                        threading.Thread(target=stop_chord_after_delay, args=(chords[hand_type][finger],), daemon=True).start()
                    prev_states[hand_type][finger] = fingers[i]  # Update state
    else:
        # If no hands detected, stop all chords after delay
        for hand in chords:
            for finger in chords[hand]:
                threading.Thread(target=stop_chord_after_delay, args=(chords[hand][finger],), daemon=True).start()
        prev_states = {hand: {finger: 0 for finger in chords[hand]} for hand in chords}
    
    return av.VideoFrame.from_ndarray(img, format="bgr24")

webrtc_streamer(
    key="hand-tracking",
    mode=WebRtcMode.SENDRECV,
    video_frame_callback=video_frame_callback,
    media_stream_constraints={"video": True, "audio": False},
    async_processing=True,
)

pygame.midi.quit()